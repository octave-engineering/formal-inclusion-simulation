{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41514,
     "status": "ok",
     "timestamp": 1759827810082,
     "user": {
      "displayName": "Micheal Olatunbosun",
      "userId": "13613313617804734234"
     },
     "user_tz": -60
    },
    "id": "RMQMuKGE4bKC",
    "outputId": "78d1102d-473b-41e4-cfc3-d6d68d77f487"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== SUMMARY ====\n",
      "{\n",
      "  \"weighted_formal_inclusion_rate\": 0.6286953866436685,\n",
      "  \"published_formal_inclusion_percent_reference\": 0.64,\n",
      "  \"best_model\": \"LogisticRegression\",\n",
      "  \"reports_dir\": \"/content/reports_excel\",\n",
      "  \"artifacts_dir\": \"/content/model_artifacts_excel\"\n",
      "}\n",
      "\n",
      "Saved:\n",
      " - Reports:        reports_excel\n",
      " - Excel bundle:   reports_excel/efina_outputs.xlsx\n",
      " - Best model:     model_artifacts_excel/best_model.joblib\n",
      " - Preprocessor:   model_artifacts_excel/preprocessor.joblib\n",
      " - Feature names:  model_artifacts_excel/feature_names.json\n"
     ]
    }
   ],
   "source": [
    "# efina_pipeline_excel.py\n",
    "\"\"\"\n",
    "EFInA Formal Inclusion — End-to-End Pipeline (Excel, robust + aligned)\n",
    "- Uses your exact headers (see YESNO_COLS / CAT_COLS_DECLARED / INCOME_COL).\n",
    "- Maps Yes/No columns to 0/1 before building the preprocessor.\n",
    "- Treats only true numeric columns as numeric (median imputation).\n",
    "- Robust Income_Level ordinal encoding.\n",
    "- Gets output feature names from the *fitted* preprocessor inside each pipeline.\n",
    "- Aligns names/importances to avoid shape errors.\n",
    "- Trains Logistic Regression, Random Forest, Gradient Boosting with EFInA sampling weights.\n",
    "- Exports Top-10 per model + Consensus Top-10, metrics, and artifacts.\n",
    "\n",
    "Run:\n",
    "    python efina_pipeline_excel.py\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,\n",
    "                             f1_score, precision_score, recall_score, roc_auc_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# =========================\n",
    "# Config — EDIT THESE\n",
    "# =========================\n",
    "EXCEL_PATH = Path(r\"/dataset/AF2023_Efina.xlsx\")  # <— change this\n",
    "SHEET_NAME = 0  # first sheet (or a string like \"Data\")\n",
    "\n",
    "RESP_ID_COL = \"respondent_serial\"\n",
    "WEIGHT_COL  = \"weighting_variable\"\n",
    "TARGET_COL  = \"Formally_Included\"   # numeric 1/0 per your description\n",
    "\n",
    "REPORT_DIR = Path(\"./reports_excel\")\n",
    "ARTIFACTS_DIR = Path(\"./model_artifacts_excel\")\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# =========================\n",
    "# Your exact columns\n",
    "# =========================\n",
    "# Categorical (small domain)\n",
    "CAT_COLS_DECLARED = [\n",
    "    \"Education\",\n",
    "    \"Gender\",\n",
    "    \"Age_Group\",\n",
    "    \"Sector\",\n",
    "]\n",
    "\n",
    "# Income (ordinal)\n",
    "INCOME_COL = \"Income_Level\"\n",
    "INCOME_ORDER = [\n",
    "    \"No income\",\n",
    "    \"Below N15,000 per month\",\n",
    "    \"N15,001 – N35,000 per month\",\n",
    "    \"N35,001 – N55,000 per month\",\n",
    "    \"N55,001 – N75,000 per month\",\n",
    "    \"N75,001 – N95,000 per month\",\n",
    "    \"N95,001 – N115,000 per month\",\n",
    "    \"N115,001 – N135,000 per month\",\n",
    "    \"N135,001 – N155,000 per month\",\n",
    "    \"N155,001 – N175,000 per month\",\n",
    "    \"N175,001 – N195,000 per month\",\n",
    "    \"N195,001 – N215,000 per month\",\n",
    "    \"N215,001 – N235,000 per month\",\n",
    "    \"N235,001 – N255,000 per month\",\n",
    "    \"N255,001 – N275,000 per month\",\n",
    "    \"N275,001 – N295,000 per month\",\n",
    "    \"N295,001 – N315,000 per month\",\n",
    "    \"Above N315,000 per month\",\n",
    "    \"Refused\",\n",
    "    \"Don't know\",  # normalized form\n",
    "]\n",
    "\n",
    "# Yes/No columns (must be mapped to 0/1)\n",
    "YESNO_COLS = [\n",
    "    \"Salary_from_Government_including_NYSC\",\n",
    "    \"Salary_Wages_From_A_Business_Company\",\n",
    "    \"Salary_Wages_From_An_Individual_With_Own_Business\",\n",
    "    \"Salary_Wages_From_An_Individual_For_Chores\",\n",
    "    \"Subsistence_Small_scale_farming\",\n",
    "    \"Commercial_Large_scale_farming\",\n",
    "    \"Own_Business_Trader_Non_farming\",\n",
    "    \"Own_Business_Trader_Farming_Produce_Livestock\",\n",
    "    \"Own_Business_Trader_Agricultural_Inputs\",\n",
    "    \"Own_Business_Provide_service\",\n",
    "    \"Rent\",\n",
    "    \"Pension\",\n",
    "    \"Government_Grant\",\n",
    "    \"Drought_Relief\",\n",
    "    \"Interest_On_Savings\",\n",
    "    \"Return_On_Investments\",\n",
    "    \"Get_Money_From_Family_Friends_Students\",\n",
    "    \"Get_Money_From_Family_Friends_Unemployed_NonStudents\",\n",
    "    \"Get_Money_From_Family_Friends_Retired\",\n",
    "    \"Financial_Service_Agent_Near_Home\",\n",
    "    \"ATM_Near_Home\",\n",
    "    \"Microfinance_Bank_Near_Home\",\n",
    "    \"Non_Interest_Service_Provider_Near_Home\",\n",
    "    \"Primary_Mortgage_Bank_Near_Home\",\n",
    "    \"Mobile_Phone\",\n",
    "    \"Reliable_Phone_Network\",\n",
    "    \"Bank_Account\",\n",
    "    \"NIN\",\n",
    "    \"BVN\",\n",
    "]\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def load_excel(path: Path, sheet_name=0) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Excel file not found at: {path}\")\n",
    "    df = pd.read_excel(path, sheet_name=sheet_name)\n",
    "    if isinstance(df, dict):  # rare guard\n",
    "        df = df[list(df.keys())[0]]\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def normalize_yes_no(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Map Yes/No-like values to 1/0. Leaves NaN as NaN.\n",
    "    Accepts many forms: Yes/No, Y/N, 1/0, True/False, etc.\n",
    "    \"\"\"\n",
    "    if pd.api.types.is_numeric_dtype(series):\n",
    "        return pd.to_numeric(series, errors=\"coerce\")\n",
    "    s = series.astype(str).str.strip().str.lower()\n",
    "    mapping = {\n",
    "        \"yes\": 1, \"y\": 1, \"1\": 1, \"true\": 1, \"t\": 1,\n",
    "        \"no\": 0,  \"n\": 0, \"0\": 0, \"false\": 0, \"f\": 0,\n",
    "        \"nan\": np.nan, \"\": np.nan, \"none\": np.nan,\n",
    "    }\n",
    "    return s.map(mapping)\n",
    "\n",
    "def ordinal_encode_income(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Encode Income_Level as ordered numeric 0..K-1.\n",
    "    Robust to variants of \"Don't/Don’t know\", capitalization, stray spaces.\n",
    "    \"\"\"\n",
    "    s = series.astype(str).str.strip()\n",
    "    # normalize curly apostrophes and variants\n",
    "    s = s.str.replace(\"\\u2019\", \"'\", regex=False)  # ’ -> '\n",
    "    s = s.str.replace(\"Don’t\", \"Don't\", regex=False)\n",
    "    s = s.str.replace(\"don’t\", \"don't\", regex=False)\n",
    "    s = s.str.replace(\"Don’t know\", \"Don't know\", regex=False)\n",
    "    s = s.str.replace(\"Don’t Know\", \"Don't know\", regex=False)\n",
    "    s = s.str.replace(\"Don’t know\", \"Don't know\", regex=False)\n",
    "    # map case-insensitively\n",
    "    allowed = {k.lower(): i for i, k in enumerate(INCOME_ORDER)}\n",
    "    out = s.str.lower().map(allowed)\n",
    "    return out.astype(\"float\")  # keep NaN\n",
    "\n",
    "def preprocess_dataframe(df: pd.DataFrame):\n",
    "    # Basic cleanup\n",
    "    df = df.replace({\"\": np.nan, \" \": np.nan})\n",
    "\n",
    "    # Required columns\n",
    "    for col in [RESP_ID_COL, WEIGHT_COL, TARGET_COL]:\n",
    "        if col not in df.columns:\n",
    "            raise KeyError(f\"Missing required column: '{col}'\")\n",
    "\n",
    "    # Map ALL declared Yes/No columns to 0/1\n",
    "    for col in YESNO_COLS:\n",
    "        if col in df.columns:\n",
    "            df[col] = normalize_yes_no(df[col])\n",
    "        else:\n",
    "            df[col] = np.nan  # create if missing\n",
    "\n",
    "    # Declared categoricals\n",
    "    for col in CAT_COLS_DECLARED:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.strip().replace({\"nan\": np.nan})\n",
    "        else:\n",
    "            df[col] = np.nan\n",
    "\n",
    "    # Income ordinal\n",
    "    if INCOME_COL in df.columns:\n",
    "        df[INCOME_COL] = ordinal_encode_income(df[INCOME_COL])\n",
    "    else:\n",
    "        df[INCOME_COL] = np.nan\n",
    "\n",
    "    # Target (should already be 0/1)\n",
    "    y = pd.to_numeric(df[TARGET_COL], errors=\"coerce\").astype(\"float\")\n",
    "\n",
    "    # Weights (normalize mean=1)\n",
    "    w = pd.to_numeric(df[WEIGHT_COL], errors=\"coerce\").fillna(1.0)\n",
    "    w = w / w.mean()\n",
    "\n",
    "    # Build feature matrix — drop id/weight/target\n",
    "    X = df.drop(columns=[c for c in [RESP_ID_COL, WEIGHT_COL, TARGET_COL] if c in df.columns])\n",
    "\n",
    "    # =========================\n",
    "    # Dynamic column typing\n",
    "    # =========================\n",
    "    # After mapping yes/no and income, re-detect types:\n",
    "    #  - Binary columns (0/1) -> numeric\n",
    "    #  - Declared categoricals -> categorical\n",
    "    #  - Any remaining object dtype -> categorical (to avoid median on strings)\n",
    "    #  - Only true numeric dtypes go to numeric pipeline\n",
    "    binary_cols = [c for c in X.columns if c in YESNO_COLS]\n",
    "    cat_cols    = [c for c in CAT_COLS_DECLARED if c in X.columns]\n",
    "    leftover_objects = [c for c in X.select_dtypes(include=[\"object\"]).columns if c not in cat_cols]\n",
    "    cat_cols += leftover_objects\n",
    "    numeric_dtypes = list(X.select_dtypes(include=[\"number\"]).columns)\n",
    "    num_cols = [c for c in numeric_dtypes if c not in set(binary_cols + [INCOME_COL])]\n",
    "\n",
    "    # Reorder X (optional)\n",
    "    ordered_cols = binary_cols + cat_cols + ([INCOME_COL] if INCOME_COL in X.columns else []) + num_cols\n",
    "    X = X[ordered_cols]\n",
    "\n",
    "    return X, y, w, binary_cols, cat_cols, [INCOME_COL] if INCOME_COL in X.columns else [], num_cols\n",
    "\n",
    "def build_preprocessor(binary_cols: List[str], cat_cols: List[str], income_cols: List[str], num_cols: List[str]) -> ColumnTransformer:\n",
    "    bin_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\"))])\n",
    "    cat_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "    ])\n",
    "    num_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\"))])\n",
    "\n",
    "    transformers = []\n",
    "    if binary_cols:\n",
    "        transformers.append((\"bin\", bin_pipe, binary_cols))\n",
    "    if cat_cols:\n",
    "        transformers.append((\"cat\", cat_pipe, cat_cols))\n",
    "    if income_cols:\n",
    "        transformers.append((\"inc\", num_pipe, income_cols))\n",
    "    if num_cols:\n",
    "        transformers.append((\"num\", num_pipe, num_cols))\n",
    "\n",
    "    pre = ColumnTransformer(transformers=transformers, remainder=\"drop\")\n",
    "    return pre\n",
    "\n",
    "def evaluate_and_report(name: str, y_true, y_prob, y_pred, weights, label: str) -> Dict:\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_prob, sample_weight=weights)\n",
    "    except Exception:\n",
    "        auc = float(\"nan\")\n",
    "    acc = accuracy_score(y_true, y_pred, sample_weight=weights)\n",
    "    f1  = f1_score(y_true, y_pred, sample_weight=weights)\n",
    "    prec = precision_score(y_true, y_pred, sample_weight=weights)\n",
    "    rec  = recall_score(y_true, y_pred, sample_weight=weights)\n",
    "    cm = confusion_matrix(y_true, y_pred, sample_weight=weights)\n",
    "    report = classification_report(y_true, y_pred, sample_weight=weights, digits=3)\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"model\": name,\n",
    "        \"auc\": auc,\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"confusion_matrix\": cm.tolist(),\n",
    "        \"report\": report\n",
    "    }\n",
    "\n",
    "def get_feature_names(pre: ColumnTransformer) -> list:\n",
    "    \"\"\"\n",
    "    Build output feature names from a *fitted* ColumnTransformer `pre`.\n",
    "    - For 'bin', 'inc', 'num' use the original column lists.\n",
    "    - For 'cat' expand via the fitted OneHotEncoder.\n",
    "    \"\"\"\n",
    "    assert hasattr(pre, \"transformers_\"), \"ColumnTransformer must be fitted before calling get_feature_names.\"\n",
    "\n",
    "    name_to_cols = {name: cols for (name, _, cols) in pre.transformers_ if cols is not None}\n",
    "    out = []\n",
    "\n",
    "    if \"bin\" in name_to_cols:\n",
    "        out.extend(list(name_to_cols[\"bin\"]))\n",
    "\n",
    "    if \"cat\" in name_to_cols:\n",
    "        cat_cols = list(name_to_cols[\"cat\"])\n",
    "        cat_tr = pre.named_transformers_[\"cat\"]\n",
    "        ohe = cat_tr.named_steps.get(\"onehot\", None) if hasattr(cat_tr, \"named_steps\") else None\n",
    "        if ohe is not None:\n",
    "            out.extend(ohe.get_feature_names_out(cat_cols).tolist())\n",
    "        else:\n",
    "            out.extend(cat_cols)\n",
    "\n",
    "    if \"inc\" in name_to_cols:\n",
    "        out.extend(list(name_to_cols[\"inc\"]))\n",
    "\n",
    "    if \"num\" in name_to_cols:\n",
    "        out.extend(list(name_to_cols[\"num\"]))\n",
    "\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "def main():\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "\n",
    "    # Load\n",
    "    df = load_excel(EXCEL_PATH, sheet_name=SHEET_NAME)\n",
    "\n",
    "    # Preprocess (robust typing)\n",
    "    X, y_float, w, bin_cols, cat_cols, income_cols, num_cols = preprocess_dataframe(df)\n",
    "\n",
    "    # Drop rows with missing target\n",
    "    mask = ~y_float.isna()\n",
    "    X = X.loc[mask].copy()\n",
    "    y = y_float.loc[mask].astype(int).values\n",
    "    w = w.loc[mask].values\n",
    "\n",
    "    # Split\n",
    "    X_tr, X_te, y_tr, y_te, w_tr, w_te = train_test_split(\n",
    "        X, y, w, test_size=0.2, stratify=y, random_state=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    # Preprocessor from detected columns\n",
    "    pre = build_preprocessor(bin_cols, cat_cols, income_cols, num_cols)\n",
    "\n",
    "    # Baseline (weighted prevalence)\n",
    "    baseline_rate = float(np.average(y_tr, weights=w_tr))\n",
    "    y_te_baseline_pred = np.full_like(y_te, 1 if baseline_rate >= 0.5 else 0)\n",
    "    y_te_baseline_prob = np.full_like(y_te, baseline_rate, dtype=float)\n",
    "    baseline_metrics = evaluate_and_report(\n",
    "        \"Weighted Prevalence Baseline\", y_te, y_te_baseline_prob, y_te_baseline_pred, w_te, \"test\"\n",
    "    )\n",
    "\n",
    "    # Models\n",
    "    models = {\n",
    "        \"LogisticRegression\": Pipeline([\n",
    "            (\"pre\", pre),\n",
    "            (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\", solver=\"liblinear\"))\n",
    "        ]),\n",
    "        \"RandomForest\": Pipeline([\n",
    "            (\"pre\", pre),\n",
    "            (\"clf\", RandomForestClassifier(\n",
    "                n_estimators=600, max_depth=None, min_samples_split=4, min_samples_leaf=1,\n",
    "                random_state=RANDOM_SEED, n_jobs=-1\n",
    "            ))\n",
    "        ]),\n",
    "        \"GradientBoosting\": Pipeline([\n",
    "            (\"pre\", pre),\n",
    "            (\"clf\", GradientBoostingClassifier(random_state=RANDOM_SEED))\n",
    "        ])\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    importances = {}\n",
    "    feature_names = None\n",
    "\n",
    "    # Train/Evaluate per model\n",
    "    for name, pipe in models.items():\n",
    "        pipe.fit(X_tr, y_tr, clf__sample_weight=w_tr)\n",
    "\n",
    "        # predictions\n",
    "        y_prob = pipe.predict_proba(X_te)[:, 1]\n",
    "        y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "        # metrics\n",
    "        res = evaluate_and_report(name, y_te, y_prob, y_pred, w_te, \"test\")\n",
    "        results.append(res)\n",
    "\n",
    "        # feature names from the *fitted* preprocessor inside this pipeline\n",
    "        pre_fitted = pipe.named_steps[\"pre\"]\n",
    "        feature_names = get_feature_names(pre_fitted)\n",
    "        n_features = len(feature_names)\n",
    "\n",
    "        # model importances\n",
    "        clf = pipe.named_steps[\"clf\"]\n",
    "        if hasattr(clf, \"feature_importances_\"):\n",
    "            importances_arr = clf.feature_importances_\n",
    "        elif hasattr(clf, \"coef_\"):\n",
    "            importances_arr = np.abs(np.ravel(clf.coef_))\n",
    "        else:\n",
    "            importances_arr = np.zeros(n_features, dtype=float)\n",
    "\n",
    "        # align lengths (safe-guard)\n",
    "        m = min(n_features, len(importances_arr))\n",
    "        feature_names = feature_names[:m]\n",
    "        importances_arr = importances_arr[:m]\n",
    "\n",
    "        # normalize + save\n",
    "        if importances_arr.sum() > 0:\n",
    "            importances_arr = importances_arr / importances_arr.sum()\n",
    "        fi = pd.DataFrame({\"feature\": feature_names, \"importance\": importances_arr}) \\\n",
    "               .sort_values(\"importance\", ascending=False)\n",
    "        importances[name] = fi\n",
    "        fi.head(10).to_csv(REPORT_DIR / f\"top10_{name}.csv\", index=False)\n",
    "\n",
    "    # Consensus Top-10\n",
    "    merged = None\n",
    "    for name, fi in importances.items():\n",
    "        tmp = fi.rename(columns={\"importance\": f\"importance_{name}\"})\n",
    "        merged = tmp if merged is None else merged.merge(tmp, on=\"feature\", how=\"outer\")\n",
    "    for name in models.keys():\n",
    "        col = f\"importance_{name}\"\n",
    "        if col not in merged.columns:\n",
    "            merged[col] = 0.0\n",
    "    merged[\"consensus_importance\"] = merged[[f\"importance_{m}\" for m in models]].mean(axis=1)\n",
    "    consensus_top10 = merged.sort_values(\"consensus_importance\", ascending=False).head(10)\n",
    "    consensus_top10.to_csv(REPORT_DIR / \"top10_consensus.csv\", index=False)\n",
    "\n",
    "    # Metrics table\n",
    "    metrics_df = pd.DataFrame(results + [baseline_metrics])\n",
    "    metrics_df.to_csv(REPORT_DIR / \"model_metrics.csv\", index=False)\n",
    "\n",
    "    # Best model by AUC then F1\n",
    "    metrics_sorted = metrics_df.sort_values([\"auc\", \"f1\"], ascending=[False, False])\n",
    "    best_name = metrics_sorted.iloc[0][\"model\"]\n",
    "    best_pipe = models[best_name]\n",
    "\n",
    "    # Save artifacts\n",
    "    joblib.dump(best_pipe, ARTIFACTS_DIR / \"best_model.joblib\")\n",
    "    joblib.dump(pre, ARTIFACTS_DIR / \"preprocessor.joblib\")\n",
    "    with open(ARTIFACTS_DIR / \"feature_names.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(get_feature_names(pre), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Summary & bundle\n",
    "    weighted_rate_full = float(np.average(y, weights=w))\n",
    "    summary = {\n",
    "        \"weighted_formal_inclusion_rate\": weighted_rate_full,\n",
    "        \"published_formal_inclusion_percent_reference\": 0.64,\n",
    "        \"best_model\": best_name,\n",
    "        \"reports_dir\": str(REPORT_DIR.resolve()),\n",
    "        \"artifacts_dir\": str(ARTIFACTS_DIR.resolve())\n",
    "    }\n",
    "    with open(REPORT_DIR / \"summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    with pd.ExcelWriter(REPORT_DIR / \"efina_outputs.xlsx\", engine=\"xlsxwriter\") as writer:\n",
    "        metrics_df.to_excel(writer, sheet_name=\"metrics\", index=False)\n",
    "        consensus_top10.to_excel(writer, sheet_name=\"top10_consensus\", index=False)\n",
    "        for name, fi in importances.items():\n",
    "            fi.head(10).to_excel(writer, sheet_name=f\"top10_{name}\", index=False)\n",
    "\n",
    "    print(\"\\n==== SUMMARY ====\")\n",
    "    print(json.dumps(summary, indent=2))\n",
    "    print(\"\\nSaved:\")\n",
    "    print(f\" - Reports:        {REPORT_DIR}\")\n",
    "    print(f\" - Excel bundle:   {REPORT_DIR / 'efina_outputs.xlsx'}\")\n",
    "    print(f\" - Best model:     {ARTIFACTS_DIR / 'best_model.joblib'}\")\n",
    "    print(f\" - Preprocessor:   {ARTIFACTS_DIR / 'preprocessor.joblib'}\")\n",
    "    print(f\" - Feature names:  {ARTIFACTS_DIR / 'feature_names.json'}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13066,
     "status": "ok",
     "timestamp": 1759827763104,
     "user": {
      "displayName": "Micheal Olatunbosun",
      "userId": "13613313617804734234"
     },
     "user_tz": -60
    },
    "id": "m9u0FIKu4g1H",
    "outputId": "64d3fe78-4911-452d-926f-b94b3f3c7551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xlsxwriter\n",
      "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
      "Downloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/175.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xlsxwriter\n",
      "Successfully installed xlsxwriter-3.2.9\n"
     ]
    }
   ],
   "source": [
    "!pip install xlsxwriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prZdnsFx6GuC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNMs0FMIQc2MCMYVJPAs7OD",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
